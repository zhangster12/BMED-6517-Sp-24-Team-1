{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "DIR_PATH = '../Consolidated_Features'\n",
    "SUBJECT_LIST = [\n",
    "    \"3128\", \"3129\", \"3130\", \"3131\", \"3132\", \"3133\", \"3136\", \"3137\", \"3138\", \"3139\", \n",
    "    \"3140\", \"3141\", \"3142\", \"3143\", \"3147\", \"3148\", \"3149\", \"3150\", \"3151\", \"3152\", \n",
    "    \"3153\", \"3154\", \"3155\", \"3156\", \"3158\", \"3159\", \"3160\", \"3162\", \"6037\", \"6038\", \n",
    "    \"6043\", \"6044\", \"6045\", \"6046\", \"6047\", \"6048\", \"6049\"\n",
    "]\n",
    "NUM_STIMULI = 5\n",
    "STIMULUS_TYPES = ['Rest', 'Reading', 'SpeechPrep', 'Speech', 'Recovery']\n",
    "NUM_FEATURES = 5\n",
    "FEATURE_TYPES = ['HR', 'PAT', 'PEP', 'PPGamp', 'PTTrecip']\n",
    "\n",
    "# List to store dataframes for MI subjects\n",
    "dataframes_mi = []\n",
    "\n",
    "# List to store dataframes for healthy control subjects\n",
    "dataframes_ht = []\n",
    "\n",
    "# List to store dataframes for all subjects\n",
    "dataframes_all = []\n",
    "\n",
    "# Variables to track index of MI, healthy control, and all subjects\n",
    "sub_mi = -1\n",
    "sub_ht = -1\n",
    "sub_all = -1\n",
    "\n",
    "# Loop through all subjects\n",
    "for sub in os.listdir(DIR_PATH):\n",
    "    subject_id = int(sub[3:])  # Extract subject ID from directory name\n",
    "\n",
    "    if str(subject_id) in SUBJECT_LIST:\n",
    "\n",
    "        # Check if the subject ID belongs to MI subjects\n",
    "        if str(subject_id).startswith('3'):\n",
    "            sub_mi += 1\n",
    "            dataframes_mi.append([])\n",
    "\n",
    "            # Loop through stimuli\n",
    "            for stim in range(NUM_STIMULI):\n",
    "                dataframes_mi[sub_mi].append([])\n",
    "\n",
    "                # Loop through features\n",
    "                for feat in range(NUM_FEATURES):\n",
    "                    dataframes_mi[sub_mi][stim].append([])\n",
    "                    feat_load = os.path.join(DIR_PATH, 'sub' + str(subject_id), 'stim' + str(stim) + '_' + FEATURE_TYPES[feat] + '.csv')\n",
    "                    data = pd.read_csv(feat_load)\n",
    "                    dataframes_mi[sub_mi][stim][feat] = data.values\n",
    "        \n",
    "        # Check if the subject ID belongs to healthy control subjects\n",
    "        elif str(subject_id).startswith('6'):\n",
    "            sub_ht += 1\n",
    "            dataframes_ht.append([])\n",
    "\n",
    "            # Loop through stimuli\n",
    "            for stim in range(NUM_STIMULI):\n",
    "                dataframes_ht[sub_ht].append([])\n",
    "\n",
    "                # Loop through features\n",
    "                for feat in range(NUM_FEATURES):\n",
    "                    dataframes_ht[sub_ht][stim].append([])\n",
    "                    feat_load = os.path.join(DIR_PATH, 'sub' + str(subject_id), 'stim' + str(stim) + '_' + FEATURE_TYPES[feat] + '.csv')\n",
    "                    data = pd.read_csv(feat_load)\n",
    "                    dataframes_ht[sub_ht][stim][feat] = data.values\n",
    "        \n",
    "        # For all subjects\n",
    "        sub_all += 1\n",
    "        dataframes_all.append([])\n",
    "\n",
    "        # Loop through stimuli\n",
    "        for stim in range(NUM_STIMULI):\n",
    "            dataframes_all[sub_all].append([])\n",
    "\n",
    "            # Loop through features\n",
    "            for feat in range(NUM_FEATURES):\n",
    "                dataframes_all[sub_all][stim].append([])\n",
    "                feat_load = os.path.join(DIR_PATH, 'sub' + str(subject_id), 'stim' + str(stim) + '_' + FEATURE_TYPES[feat] + '.csv')\n",
    "                data = pd.read_csv(feat_load)\n",
    "                dataframes_all[sub_all][stim][feat] = data.values\n",
    "\n",
    "print(\"Data loading completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unhealthy vs. healthy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine MI (Myocardial Infarction) vs. HT (Healthy Control) status based on subject identifier\n",
    "mi_subjects = [\"3128\", \"3129\", \"3130\", \"3131\", \"3132\", \"3133\", \"3136\", \"3137\", \"3138\", \"3139\", \n",
    "               \"3140\", \"3141\", \"3142\", \"3143\", \"3147\", \"3148\", \"3149\", \"3150\", \"3151\", \"3152\", \n",
    "               \"3153\", \"3154\", \"3155\", \"3156\", \"3158\", \"3159\", \"3160\", \"3162\"]\n",
    "\n",
    "# Healthy Control subjects\n",
    "ht_subjects = [\"6037\", \"6038\", \"6043\", \"6044\", \"6045\", \"6046\", \"6047\", \"6048\", \"6049\"]\n",
    "\n",
    "# Initialization of lists to store health condition labels\n",
    "health_condition_labels = []\n",
    "\n",
    "# Modify the loop to assign labels based on health condition\n",
    "for subj_index, subj_id in enumerate(SUBJECT_LIST):\n",
    "\n",
    "    if subj_id in mi_subjects:\n",
    "        health_condition = 0  # Myocardial Infarction\n",
    "\n",
    "    elif subj_id in ht_subjects:\n",
    "        health_condition = 1  # Healthy Control\n",
    "        \n",
    "    else:\n",
    "        health_condition = \"Unknown\"\n",
    "    \n",
    "    # Create labels based on health condition for all stimuli and features\n",
    "    subj_labels = []\n",
    "\n",
    "    for _ in range(NUM_STIMULI):\n",
    "        stim_labels = []\n",
    "\n",
    "        for _ in range(NUM_FEATURES):\n",
    "            stim_labels.append(health_condition)\n",
    "\n",
    "        subj_labels.append(stim_labels)\n",
    "        \n",
    "    health_condition_labels.append(subj_labels)\n",
    "\n",
    "# Now, health_condition_labels contains labels for each subject, stimulus, and feature based on health condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dyzha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\dyzha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret / rcount\n"
     ]
    }
   ],
   "source": [
    "# %% Baseline correction (normalization)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store mean difference, normalized data, and labels\n",
    "mean_difference_all = []  # Stores the mean difference between feature and baseline for all subjects, stimuli, and features\n",
    "normalized_data_all = []  # Stores the normalized data for all subjects, stimuli, and features\n",
    "labels_all = []  # Stores the labels for all subjects, stimuli, and features\n",
    "\n",
    "# Number of data points and half of it\n",
    "num_data_points = 80  # Total number of data points\n",
    "data_points_half = round(num_data_points / 2)  # Half of the total number of data points\n",
    "\n",
    "# Loop through each subject\n",
    "for subject_index, subject_id in enumerate(SUBJECT_LIST):\n",
    "    normalized_data_subject = []  # Stores the normalized data for the current subject\n",
    "    labels_subject = []  # Stores the labels for the current subject\n",
    "\n",
    "    # Loop through each stimulus\n",
    "    for stimulus_index in range(NUM_STIMULI):\n",
    "        normalized_data_stimulus = []  # Stores the normalized data for the current stimulus\n",
    "        labels_stimulus = []  # Stores the labels for the current stimulus\n",
    "\n",
    "        # Loop through each feature\n",
    "        for feature_index in range(NUM_FEATURES):\n",
    "            baseline_data = dataframes_all[subject_index][0][feature_index][:, 1]  # Baseline data\n",
    "            baseline_midpoint = baseline_data[len(baseline_data) // 2 - data_points_half:len(baseline_data) // 2 + data_points_half]  # Midpoint of baseline data\n",
    "            feature_data = dataframes_all[subject_index][stimulus_index][feature_index][:, 1]  # Feature data\n",
    "            feature_midpoint = feature_data[len(feature_data) // 2 - data_points_half:len(feature_data) // 2 + data_points_half]  # Midpoint of feature data\n",
    "\n",
    "            # Calculate mean difference between feature and baseline\n",
    "            mean_difference = (np.mean(feature_midpoint) - np.mean(baseline_midpoint)) / np.mean(baseline_midpoint)\n",
    "            mean_difference_all.append(mean_difference)\n",
    "\n",
    "            # Normalize the data and assign labels\n",
    "            if stimulus_index == 0:\n",
    "                # Baseline data\n",
    "                normalized_data = (baseline_midpoint - np.mean(baseline_midpoint)) / np.mean(baseline_midpoint)\n",
    "                labels = np.zeros_like(normalized_data)  # Assign label 0\n",
    "\n",
    "            else:\n",
    "                # Feature data\n",
    "                normalized_data = (feature_midpoint - np.mean(baseline_midpoint)) / np.mean(baseline_midpoint)\n",
    "                labels = np.ones_like(normalized_data)  # Assign label 1\n",
    "\n",
    "            normalized_data_stimulus.append(normalized_data)\n",
    "            labels_stimulus.append(labels)\n",
    "\n",
    "        normalized_data_subject.append(normalized_data_stimulus)\n",
    "        labels_subject.append(labels_stimulus)\n",
    "\n",
    "    normalized_data_all.append(normalized_data_subject)\n",
    "    labels_all.append(labels_subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get normalized data and labels for SpeechPrep for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_prep_normalized_all = [subject_data[STIMULUS_TYPES.index('SpeechPrep')] for subject_data in normalized_data_all]\n",
    "speech_prep_labels_all = [subject_labels[STIMULUS_TYPES.index('SpeechPrep')] for subject_labels in health_condition_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29590725 -0.23432916  0.79120646 -0.26154978 -0.23256721 -0.27685996\n",
      " -0.20664846 -0.05735781 -0.14445169  0.02355272 -0.20791483 -0.24472829\n",
      " -0.30319517 -0.19210061 -0.10172878 -0.24173319 -0.17631923 -0.10005035\n",
      " -0.25415563  0.23227604 -0.12363237 -0.26194479 -0.18723549  0.03106238\n",
      " -0.0616728  -0.10754111 -0.09316412 -0.18023539 -0.09381698 -0.25121662\n",
      "  0.69707204 -0.46894726 -0.02349415 -0.1667557  -0.294304   -0.20547098\n",
      " -0.25146282 -0.00695033 -0.27949367  0.03463711 -0.19162935 -0.17926633\n",
      " -0.28882227  0.09070124 -0.29293056  0.33252426 -0.06158022 -0.27144478\n",
      " -0.23731542 -0.02565898 -0.31054077  0.3310088  -0.23950125 -0.2822939\n",
      " -0.02195363 -0.22338726  0.78492347 -0.14956874 -0.29236288 -0.08351126\n",
      " -0.23574644  0.1843918  -0.12208237 -0.13490612 -0.04912538 -0.07359189\n",
      " -0.14817531  0.24448738 -0.20791303 -0.25326048 -0.2046241  -0.1743267\n",
      "  0.05188645  0.09576723 -0.06546477 -0.10565629 -0.05546834 -0.30825671\n",
      "  0.56431492 -0.16803683]\n"
     ]
    }
   ],
   "source": [
    "print(speech_prep_normalized_all[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aggregated data and labels specifically for the \"Speech Prep\" stimulus\n",
    "aggregated_data_speech_prep = []  # Stores the aggregated normalized data for all features for \"Speech Prep\"\n",
    "aggregated_labels_speech_prep = []  # Stores the aggregated labels for all features for \"Speech Prep\"\n",
    "\n",
    "for feature_index in range(NUM_FEATURES):\n",
    "    aggregated_data_feature = []  # Temporary list to aggregate normalized data for the current feature for \"Speech Prep\"\n",
    "    aggregated_labels_feature = []  # Temporary list to aggregate labels for the current feature for \"Speech Prep\"\n",
    "\n",
    "    # Loop through each subject\n",
    "    for subject_index in range(len(SUBJECT_LIST)):\n",
    "        # Extend the temporary lists with data and labels for the current feature and subject for \"Speech Prep\"\n",
    "        aggregated_data_feature.extend(normalized_data_all[subject_index][2][feature_index])  # Assuming index 2 corresponds to \"Speech Prep\"\n",
    "        aggregated_labels_feature.extend(labels_all[subject_index][2][feature_index])  # Assuming index 2 corresponds to \"Speech Prep\"\n",
    "\n",
    "    # Append the aggregated data and labels for the current feature to the overall lists for \"Speech Prep\"\n",
    "    aggregated_data_speech_prep.append(aggregated_data_feature)\n",
    "    aggregated_labels_speech_prep.append(aggregated_labels_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the minimum length among all lists in data_all_1\n",
    "min_length_data = min(len(lst) for lst in aggregated_data_speech_prep)\n",
    "\n",
    "# Truncate each inner list in data_all_1 to have the minimum length\n",
    "data_all_1_truncated = [lst[:min_length_data] for lst in aggregated_data_speech_prep]\n",
    "\n",
    "# Convert the truncated list of lists to a Numpy array\n",
    "data_all_1_array = np.array(data_all_1_truncated)\n",
    "\n",
    "# Reshape the Numpy array to have 5 columns\n",
    "num_columns = 5\n",
    "data_all_1_array = data_all_1_array.reshape(-1, num_columns)\n",
    "\n",
    "# Determine the minimum length among all lists in data_y_all_1\n",
    "min_length_data_y = min(len(lst) for lst in aggregated_labels_speech_prep)\n",
    "\n",
    "# Truncate each inner list in data_y_all_1 to have the minimum length\n",
    "data_y_all_1_truncated = [lst[:min_length_data_y] for lst in aggregated_labels_speech_prep]\n",
    "\n",
    "# Convert the truncated list of lists to a Numpy array\n",
    "data_y_all_1_array = np.array(data_y_all_1_truncated)\n",
    "\n",
    "# Reshape the Numpy array to have 5 columns\n",
    "data_y_all_1_array = data_y_all_1_array.reshape(-1, num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import SparsePCA, KernelPCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feats = data_all_1_array[:, :]\n",
    "X = feats\n",
    "labels = data_y_all_1_array[:, 0]\n",
    "y = labels\n",
    "\n",
    "lPCA = PCA(n_components = 2)\n",
    "lPCA.fit(X)\n",
    "X_PCA = lPCA.transform(X)\n",
    "\n",
    "sPCA = SparsePCA(n_components = 2)\n",
    "sPCA.fit(X)\n",
    "X_sPCA = sPCA.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28299235 -0.26551306]\n",
      " [ 0.28160219 -0.23429045]\n",
      " [ 0.3632517  -0.26207517]\n",
      " ...\n",
      " [-0.54943567  0.45378652]\n",
      " [-0.15883227 -0.36080827]\n",
      " [ 0.31802296  0.80174552]]\n"
     ]
    }
   ],
   "source": [
    "print(X_PCA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
